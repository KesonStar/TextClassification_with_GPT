{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer  \n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preparation\n",
    "## 0.1 GPT Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 'embd_size' sized vector divided into 'num_heads' heads\n",
    "        assert config.embd_size % config.num_heads == 0, f\"embedding dim should be divisible by number of heads\"\n",
    "        self.num_heads = config.num_heads\n",
    "        self.embd_size = config.embd_size\n",
    "        # batched key, query, and value projections for all heads\n",
    "        self.c_attn = nn.Linear(config.embd_size, 3 * config.embd_size)\n",
    "        self.c_proj = nn.Linear(config.embd_size, config.embd_size)\n",
    "        self.c_proj.SCALE_INIT = 1.0\n",
    "        # not really a bias, more of a mask, but following OpenAI/HF naming convention\n",
    "        # self.register_buffer(\"bias\", torch.tril(torch.ones(config.context_length, config.context_length)).view(1, 1, config.context_length, config.context_length))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "     \n",
    "        qkv = self.c_attn(x)    # (B, T, 3C)\n",
    "        q, k, v = qkv.split(self.embd_size, dim=-1)    # (B,T,C), (B,T,C), (B,T,C)\n",
    "        q = q.view(B, T, self.num_heads, self.embd_size // self.num_heads).transpose(1, 2)    # (B,nh,T,hs)\n",
    "        k = k.view(B, T, self.num_heads, self.embd_size // self.num_heads).transpose(1, 2)    # (B,nh,T,hs)\n",
    "        v = v.view(B, T, self.num_heads, self.embd_size // self.num_heads).transpose(1, 2)    # (B,nh,T,hs)\n",
    "    \n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal=True)    # (B,nh,T,hs)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)    # (B,nh,T,hs) --> (B,T,nh,hs) --> (B,T,C=nh*hs)\n",
    "        out = self.c_proj(out)    # (B,T,C) --> (B,T,C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.embd_size, 4 * config.embd_size)\n",
    "        self.gelu = nn.GELU(approximate='tanh')    # approximate='tanh' used to try to reproduce gpt2 paper\n",
    "        self.c_proj = nn.Linear(4 * config.embd_size, config.embd_size)\n",
    "        self.c_proj.SCALE_INIT = 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer Encoder block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.embd_size)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.embd_size)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(self.config.vocab_size, self.config.embd_size),\n",
    "            wpe = nn.Embedding(self.config.context_length, self.config.embd_size),\n",
    "            h = nn.ModuleList([Block(self.config) for _ in range(self.config.num_layers)]),\n",
    "            ln_f = nn.LayerNorm(self.config.embd_size)\n",
    "        ))\n",
    "        # language modeling head\n",
    "        self.lm_head = nn.Linear(self.config.embd_size, self.config.vocab_size, bias=False)\n",
    "        # weight sharing scheme (reduces 768*50267=~40M params, fewer params, more efficient)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        # init params (iterates over all submodules and applies _init_weights)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'SCALE_INIT'):\n",
    "                std /= (2 * self.config.num_layers)**0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0, std=std)    # as per openai gpt-2 source code\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.config.context_length, f'sequence length {T} should be <= {self.config.context_length}'\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)    # (T,)\n",
    "        pos_embd = self.transformer.wpe(pos)    # (T, embd_size)\n",
    "        tok_embd = self.transformer.wte(idx)    # (B, T, embd_size)\n",
    "        x = pos_embd + tok_embd    # (B, T, embd_size)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)    # (B, T, embd_size)\n",
    "        logits = self.lm_head(x)    # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, input_ids, max_length=65): ## only to generate the <好评> or <差评>\n",
    "        gen_tokens = input_ids\n",
    "        while gen_tokens.shape[-1] < max_length:\n",
    "            with torch.inference_mode():\n",
    "                logits, loss = self(gen_tokens.to(device))\n",
    "                logits = logits[:, -1, :]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_tok = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "                gen_tokens = torch.cat([gen_tokens, next_tok], dim=-1)\n",
    "        return gen_tokens\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\" Loads pretrained GPT2 model weights from huggingface \"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'uer/gpt2-distil-chinese-cluecorpussmall'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(f\"loading weights from pretrained gpt: {model_type}\")\n",
    "\n",
    "        config_args = {\n",
    "            'uer/gpt2-distil-chinese-cluecorpussmall': dict(num_layers=6, num_heads=12, embd_size=768),    # 82M params\n",
    "            'gpt2': dict(num_layers=12, num_heads=12, embd_size=768),    # 124M params\n",
    "            'gpt2-medium': dict(num_layers=24, num_heads=16, embd_size=1024),    # 350M params\n",
    "            'gpt2-large': dict(num_layers=36, num_heads=20, embd_size=1280),    # 774M params\n",
    "            'gpt2-xl': dict(num_layers=48, num_heads=25, embd_size=1600),    # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 21128\n",
    "        config_args['context_length'] = 1024\n",
    "\n",
    "        # create a from-scratch minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPTLM(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n",
    "\n",
    "        # init a huggingface transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "\n",
    "        assert len(sd_keys) == len(sd_keys_hf), f\"mismatched keys {len(sd_keys)} != {len(sd_keys_hf)}\"\n",
    "\n",
    "        # copy while ensuring all parameters are aligned in names and shape\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # need to transpose Conv1D weights\n",
    "                assert sd_hf[k].shape[::-1]  == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].T)\n",
    "            else:\n",
    "                # print(k, sd_hf[k].shape, sd[k].shape)\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Tokenizer, Dataset and Dataloader\n",
    "1. We use **BertTokenizer** found on *huggingface* in order to tokenize Chinese text.\n",
    "2. In order to better classify the sentiment, we add four special tokens to the beginning of each text:`<好评>`, `<差评>`, `<review>`, `<sentiment>`.\n",
    "3. We may find out that in the dataset, bias is quite common, where the positive samples are much more than the negative samples(4305-695). In order to balance the dataset, we simply **over-sample** the negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21132"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\")\n",
    "\n",
    "## Important!!!!! Add special tokens in order to train the model\n",
    "\n",
    "special_tokens_dict = {\"additional_special_tokens\": [\"<好评>\", \"<差评>\", \"<review>\", \"<sentiment>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./reviews.csv\")\n",
    "\n",
    "\n",
    "df_positive = df[df['sentiment'] == 1]\n",
    "df_negative = df[df['sentiment'] == 0]\n",
    "n_positive = len(df_positive)\n",
    "\n",
    "df_negative_over = df_negative.sample(n=n_positive, replace=True, random_state=42)\n",
    "df = pd.concat([df_positive, df_negative_over])\n",
    "\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sample(review_text, label, tokenizer):\n",
    "    label_str = \"<好评>\" if label == 1 else \"<差评>\"\n",
    "    prompt = f\"<review>\\n{review_text}\\n<sentiment>\\n\"\n",
    "    full_text = prompt + label_str\n",
    "\n",
    "    encoding = tokenizer(full_text,\n",
    "                         return_tensors=\"pt\",\n",
    "                         padding=\"max_length\",  \n",
    "                         max_length=64,\n",
    "                         truncation=True, add_special_tokens=False)\n",
    "    \n",
    "    input_ids = encoding.input_ids.squeeze(0)\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # 计算 prompt 部分的 token 数量，设置为 -100 来屏蔽其 loss\n",
    "    prompt_encoding = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    prompt_length = prompt_encoding.input_ids.shape[1]\n",
    "    labels[:prompt_length] = -100 # set to -100 to ignore loss\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"prompt_length\": prompt_length\n",
    "    }\n",
    "    \n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer=None):\n",
    "        self.df = dataframe\n",
    "        if tokenizer is not None:\n",
    "            self.tokenizer = tokenizer\n",
    "        else:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\")\n",
    "            self.tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<好评>\", \"<差评>\", \"<review>\", \"<sentiment>\"]})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        review = self.df.iloc[index]['review']\n",
    "        sentiment = self.df.iloc[index]['sentiment']  # 1 or 0\n",
    "\n",
    "     \n",
    "        return preprocess_sample(review, sentiment, self.tokenizer)\n",
    "\n",
    "class ValidDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer=None):\n",
    "        self.df = dataframe\n",
    "        if tokenizer is not None:\n",
    "            self.tokenizer = tokenizer\n",
    "        else:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\")\n",
    "            self.tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<好评>\", \"<差评>\", \"<review>\", \"<sentiment>\"]})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        review_text = self.df.iloc[index]['review']\n",
    "        label = self.df.iloc[index]['sentiment']  # 1 or 0\n",
    "        label_str = \"<好评>\" if label == 1 else \"<差评>\"\n",
    "        prompt = f\"<review>\\n{review_text}\\n<sentiment>\\n\"\n",
    "        # full_text = prompt + label_str\n",
    "\n",
    "        encoding = tokenizer(prompt,\n",
    "                            return_tensors=\"pt\",\n",
    "                            padding=\"max_length\",  \n",
    "                            max_length=64,\n",
    "                            truncation=True, add_special_tokens=False)\n",
    "        \n",
    "        input_ids = encoding.input_ids.squeeze(0) \n",
    "        \n",
    "        attention_mask = encoding.attention_mask.squeeze(0)\n",
    "\n",
    "        # 计算 prompt 部分的 token 数量，设置为 -100 来屏蔽其 loss\n",
    "        labels = tokenizer(label_str, return_tensors=\"pt\", add_special_tokens=False).input_ids.squeeze(0)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TrainDataset(train_df, tokenizer)\n",
    "validset = ValidDataset(val_df, tokenizer)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "validloader = DataLoader(validset, batch_size=64, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. GPT-2 with autoregressive \n",
    "## 1.1 Train a tiny toy GPT-2 model\n",
    "**Hyperparams are listed as follows**\n",
    "1. `num_layers`: 2\n",
    "2. `hidden_size`: 384\n",
    "3. `num_attention_heads`: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    context_length: int = 128   # max context / sequence length\n",
    "    vocab_size: int = 21132    # number of tokens for bert-tokenizer + special tokens\n",
    "    # num_layers: int = 6\n",
    "    num_layers: int = 2\n",
    "    # embd_size: int = 768   # embedding dim\n",
    "    embd_size: int = 384\n",
    "    num_heads: int = 2\n",
    "    hidden_size: int = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig()\n",
    "model = GPTLM(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    total_samples = 0\n",
    "    correct_preds = 0\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in validloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            batch_size = input_ids.size(0)\n",
    "            actual_lengths = attention_mask.sum(dim=1) \n",
    " \n",
    "            for i in range(batch_size):\n",
    "                \n",
    "                prompt_len = int(actual_lengths[i].item())\n",
    "                single_input_ids = input_ids[i][:prompt_len].unsqueeze(0)\n",
    "                single_mask = attention_mask[i][:prompt_len].unsqueeze(0)\n",
    "                generated_ids = model.generate(single_input_ids, max_length=single_input_ids.shape[1]+1)\n",
    "                answer_ids = generated_ids[0, prompt_len:prompt_len+1]\n",
    "\n",
    "                # print(f\"generated_ids: {generated_ids}\")\n",
    "                # print(f\"answer_ids: {answer_ids}\")\n",
    "                # print(f\"labels: {labels[i]}\")\n",
    "\n",
    "    \n",
    "                true_text = labels[i]\n",
    "                # print(f\"answer_ids: {answer_ids}, true_text: {true_text}\")\n",
    "     \n",
    "                total_samples += 1\n",
    "                \n",
    "                # 这里简单判断：如果预测结果中包含真实标签，就认为预测正确\n",
    "                if true_text == answer_ids:\n",
    "                    correct_preds += 1\n",
    "                # if total_samples >= 200:\n",
    "                #     return correct_preds / total_samples\n",
    "                \n",
    "    accuracy = correct_preds / total_samples if total_samples > 0 else 0.0\n",
    "    model.train()\n",
    "    return accuracy\n",
    "            \n",
    "def train(model, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in trange(num_epochs):\n",
    "\n",
    "        for step, batch in enumerate(trainloader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits, loss = model(input_ids, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if step % 50 == 0:\n",
    "                print(f\"Epoch: {epoch} Step {step}: Loss = {loss.item():.6f}\")\n",
    "        \n",
    "        eval_accuracy = evaluate(model)\n",
    "        print(f\"Validation Accuracy: {eval_accuracy:.4f}\")\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Step 0: Loss = 8.574745\n",
      "Epoch: 0 Step 50: Loss = 1.433478\n",
      "Epoch: 0 Step 100: Loss = 0.573784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:04<00:09,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5319\n",
      "Epoch: 1 Step 0: Loss = 0.393401\n",
      "Epoch: 1 Step 50: Loss = 0.199039\n",
      "Epoch: 1 Step 100: Loss = 0.118902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:09<00:04,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5517\n",
      "Epoch: 2 Step 0: Loss = 0.099751\n",
      "Epoch: 2 Step 50: Loss = 0.069668\n",
      "Epoch: 2 Step 100: Loss = 0.052152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:13<00:00,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5470\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "train(model, optimizer, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"toy-gpt2-5470\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review_text, model, tokenizer, device, gen_max_length=1):\n",
    "    prompt = f\"<review>\\n{review_text}\\n<sentiment>\\n\"\n",
    "    \n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, add_special_tokens=False)\n",
    "    input_ids = encoding.input_ids.to(device)\n",
    "    print(input_ids)\n",
    "\n",
    "    # print(attention_mask)\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=input_ids.shape[1] + gen_max_length,\n",
    "    )\n",
    "\n",
    "    print(output_ids)\n",
    "    generated_tokens = output_ids[0][input_ids.shape[1]:]\n",
    "\n",
    "    generated_text = tokenizer.decode(generated_tokens)\n",
    "    \n",
    "    return generated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[21130,  4162,  4275,  8013,  8013,  8013, 21131]], device='cuda:0')\n",
      "tensor([[21130,  4162,  4275,  8013,  8013,  8013, 21131, 21129]],\n",
      "       device='cuda:0')\n",
      "Prediction: <差评>\n"
     ]
    }
   ],
   "source": [
    "comment = \"烂片！！！\"\n",
    "model.eval()\n",
    "result = predict_sentiment(comment, model, tokenizer, device)\n",
    "print(\"Prediction:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Post-train a original GPT-2 117M(GPT small) with pretrained weights.\n",
    "> In this part, we first load the pretrained weights from huggingface and extract the weights of the model. Then we load the weights into our own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    context_length: int = 1024   # max context / sequence length\n",
    "    vocab_size: int = 21132    # number of tokens for bert-tokenizer + special tokens\n",
    "    num_layers: int = 2\n",
    "    # embd_size: int = 768   # embedding dim\n",
    "    embd_size: int = 768\n",
    "    num_heads: int = 12\n",
    "    hidden_size: int = 768\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(21132, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=21132, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    total_samples = 0\n",
    "    correct_preds = 0\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in validloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            batch_size = input_ids.size(0)\n",
    "            actual_lengths = attention_mask.sum(dim=1) \n",
    " \n",
    "            for i in range(batch_size):\n",
    "                \n",
    "                prompt_len = int(actual_lengths[i].item())\n",
    "                single_input_ids = input_ids[i][:prompt_len].unsqueeze(0)\n",
    "                single_mask = attention_mask[i][:prompt_len].unsqueeze(0)\n",
    "                generated_ids = model.generate(single_input_ids, attention_mask = single_mask, max_length=64+1, pad_token_id=0, do_sample=False)\n",
    "                answer_ids = generated_ids[0, prompt_len:prompt_len+1]\n",
    "\n",
    "                # print(f\"generated_ids: {generated_ids}\")\n",
    "                # print(f\"answer_ids: {answer_ids}\")\n",
    "                # print(f\"labels: {labels[i]}\")\n",
    "\n",
    "                # 获取当前样本的真实标签文本\n",
    "                true_text = labels[i]\n",
    "                # print(f\"answer_ids: {answer_ids}, true_text: {true_text}\")\n",
    "     \n",
    "                total_samples += 1\n",
    "                \n",
    "                if true_text == answer_ids:\n",
    "                    correct_preds += 1\n",
    "                if total_samples >= 200:\n",
    "                    return correct_preds / total_samples\n",
    "                \n",
    "    accuracy = correct_preds / total_samples if total_samples > 0 else 0.0\n",
    "    return accuracy\n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 10.38696480\n",
      "Step 50: Loss = 0.02465775\n",
      "Step 100: Loss = 0.01558975\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:41<06:10, 41.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6250\n",
      "Step 0: Loss = 0.01364681\n",
      "Step 50: Loss = 0.01095307\n",
      "Step 100: Loss = 0.00756635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:59<03:42, 27.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 0.01118568\n",
      "Step 50: Loss = 0.00486148\n",
      "Step 100: Loss = 0.00687969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:18<02:45, 23.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 0.00792512\n",
      "Step 50: Loss = 0.00663822\n",
      "Step 100: Loss = 0.00481947\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [02:01<03:06, 31.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8250\n",
      "Step 0: Loss = 0.00388948\n",
      "Step 50: Loss = 0.00548437\n",
      "Step 100: Loss = 0.00470487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [02:19<02:13, 26.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 0.00419920\n",
      "Step 50: Loss = 0.00166438\n",
      "Step 100: Loss = 0.00204354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [02:38<01:36, 24.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 0.00154979\n",
      "Step 50: Loss = 0.00122290\n",
      "Step 100: Loss = 0.00309407\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [03:23<01:32, 31.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8950\n",
      "Step 0: Loss = 0.00078810\n",
      "Step 50: Loss = 0.00114134\n",
      "Step 100: Loss = 0.00068413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [03:42<00:54, 27.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 0.00026817\n",
      "Step 50: Loss = 0.00019991\n",
      "Step 100: Loss = 0.00161236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [04:02<00:24, 24.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 0.00041864\n",
      "Step 50: Loss = 0.00069275\n",
      "Step 100: Loss = 0.00043163\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:50<00:00, 29.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9100\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import trange\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 10\n",
    "for epoch in trange(num_epochs):\n",
    "    for step, batch in enumerate(trainloader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}: Loss = {loss.item():.8f}\")\n",
    "    if epoch % 3 == 0:\n",
    "        print(\"Evaluating...\")\n",
    "        eval_accuracy = evaluate(model)\n",
    "        print(f\"Validation Accuracy: {eval_accuracy:.4f}\")\n",
    "        model.train()\n",
    "            \n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"GPT2-small.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained GPT-2 weights from GPT2-small.pt...\n",
      "Checkpoint keys loaded: 77\n",
      "Filtered checkpoint keys: 77, Model keys: 149\n",
      "Weights successfully loaded!\n",
      "Model is ready for inference.\n"
     ]
    }
   ],
   "source": [
    "config = GPTConfig()\n",
    "model = GPTLM(config)\n",
    "\n",
    "\n",
    "def load_pretrained_gpt2(model_path: str):\n",
    "    \"\"\"\n",
    "    从本地的 Hugging Face GPT-2 预训练权重加载到自定义的 GPTLM 模型中。\n",
    "    :param model_path: 本地的 GPT-2 预训练模型文件路径（.pt 文件）\n",
    "    :return: 加载完毕的 GPTLM 模型\n",
    "    \"\"\"\n",
    "\n",
    "    # 1️ 加载 Hugging Face 格式的 .pt 预训练权重\n",
    "    print(f\"Loading pretrained GPT-2 weights from {model_path}...\")\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "    # 确保 checkpoint 是 state_dict\n",
    "    if \"state_dict\" in checkpoint:\n",
    "        checkpoint = checkpoint[\"state_dict\"]  # 提取权重部分\n",
    "\n",
    "    print(f\"Checkpoint keys loaded: {len(checkpoint.keys())}\")\n",
    "\n",
    "    # 2️ 定义你的 GPTLM 模型\n",
    "    config_args = dict(\n",
    "        num_layers=12,    # 层数\n",
    "        num_heads=12,     # 头数\n",
    "        embd_size=768,    # 隐藏层维度\n",
    "        vocab_size=21132, # 词汇大小（确保和 tokenizer 匹配）\n",
    "        context_length=1024  # 最大序列长度\n",
    "    )\n",
    "    \n",
    "    config = GPTConfig(**config_args)\n",
    "    model = GPTLM(config)\n",
    "\n",
    "    # 3️ 获取自定义模型的 state_dict\n",
    "    model_sd = model.state_dict()\n",
    "    \n",
    "    # 过滤掉 Hugging Face 中不需要的 keys，如 attn.bias 和 masked_bias\n",
    "    checkpoint = {k: v for k, v in checkpoint.items() if not k.endswith('.attn.bias') and not k.endswith('.attn.masked_bias')}\n",
    "    \n",
    "    # 确保 key 数量匹配\n",
    "    print(f\"Filtered checkpoint keys: {len(checkpoint.keys())}, Model keys: {len(model_sd.keys())}\")\n",
    "\n",
    "    # 4️ 处理 Hugging Face 中的 Conv1D 层（需要转置）\n",
    "    transposed_layers = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "    \n",
    "    for k in checkpoint:\n",
    "        if k in model_sd:\n",
    "            if any(k.endswith(w) for w in transposed_layers):\n",
    "                # 转置 Hugging Face 的 Conv1D 层权重\n",
    "                checkpoint[k] = checkpoint[k].T\n",
    "\n",
    "    # 5️ 加载权重到模型\n",
    "    model.load_state_dict(checkpoint, strict=False)  # strict=False 允许部分 key 不匹配\n",
    "    print(\"Weights successfully loaded!\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    print(\"Model is ready for inference.\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "gpt_model = load_pretrained_gpt2(\"GPT2-small.pt\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[21130,   852,  3221,  1922,  4162,   749,  8024,  1922,  3472,   749,\n",
      "         21131]], device='cuda:0')\n",
      "tensor([[21130,   852,  3221,  1922,  4162,   749,  8024,  1922,  3472,   749,\n",
      "         21131, 21128]], device='cuda:0')\n",
      "预测结果: <好评>\n"
     ]
    }
   ],
   "source": [
    "comment = \"但是太烂了，太棒了\"\n",
    "gpt_model.eval()\n",
    "result = predict_sentiment(comment, gpt_model, tokenizer, device)\n",
    "print(\"预测结果:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8861788617886179"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    total_samples = 0\n",
    "    correct_preds = 0\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in validloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            batch_size = input_ids.size(0)\n",
    "            actual_lengths = attention_mask.sum(dim=1) \n",
    " \n",
    "            for i in range(batch_size):\n",
    "                \n",
    "                prompt_len = int(actual_lengths[i].item())\n",
    "                single_input_ids = input_ids[i][:prompt_len].unsqueeze(0)\n",
    "                single_mask = attention_mask[i][:prompt_len].unsqueeze(0)\n",
    "                generated_ids = model.generate(single_input_ids, max_length=single_input_ids.shape[1]+1)\n",
    "                # 筛选出生成的部分：从实际 prompt 长度位置开始，直到生成结束\n",
    "                answer_ids = generated_ids[0, prompt_len:prompt_len+1]\n",
    "\n",
    "                # print(f\"generated_ids: {generated_ids}\")\n",
    "                # print(f\"answer_ids: {answer_ids}\")\n",
    "                # print(f\"labels: {labels[i]}\")\n",
    "                true_text = labels[i]\n",
    "                # print(f\"answer_ids: {answer_ids}, true_text: {true_text}\")\n",
    "     \n",
    "                total_samples += 1\n",
    "   \n",
    "                if true_text == answer_ids:\n",
    "                    correct_preds += 1\n",
    "                # if total_samples >= 200:\n",
    "                #     return correct_preds / total_samples\n",
    "                \n",
    "    accuracy = correct_preds / total_samples if total_samples > 0 else 0.0\n",
    "    model.train()\n",
    "    return accuracy\n",
    "            \n",
    "evaluate(gpt_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. GPT-2 with classification head\n",
    "> This part is a another try which is mentioned in GPT-1 paper. In this part, the GPT model works as a feature extractor to extract the features of the text, and then we add a classification head to classify the sentiment.\n",
    "\n",
    "Note that in this part, we do not add the special tokens to tokenizers. Instead, we use the **last hidden state of the model** to work as a **encoder**.\n",
    "\n",
    "1. We can see that with the help of the **classification head**, the model can achieve a better performance than the autoregressive model.\n",
    "2. Only train the classification with the pretrained weights, the model performs poor(only a little better than random guess). This is maybe because of the **feature extractor** is not good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7749, 861)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPTDataset(Dataset):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.data = df\n",
    "        self.data = self.data.dropna()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data.iloc[index][\"review\"], self.data.iloc[index][\"sentiment\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "dataset = GPTDataset()\n",
    "trainset, validset = random_split(dataset, lengths=[0.9, 0.1])\n",
    "len(trainset), len(validset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_func(batch):\n",
    "    texts, sentiments = [], []\n",
    "    for text, sentiment in batch:\n",
    "        texts.append(text)\n",
    "        sentiments.append(sentiment)\n",
    "        \n",
    "    inputs = tokenizer(texts, max_length = 64, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    inputs['sentiments'] = torch.tensor(sentiments)\n",
    "    # inputs.pop(\"token_type_ids\")\n",
    "    # inputs.pop(\"attention_mask\")\n",
    "    return inputs\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=collate_func)\n",
    "validloader = DataLoader(validset, batch_size=64, shuffle=False, collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    context_length: int = 1024    # max context / sequence length\n",
    "    vocab_size: int = 21128    # number of tokens for bert-tokenizer\n",
    "    num_layers: int = 12\n",
    "    embd_size: int = 768    # embedding dim\n",
    "    num_heads: int = 12\n",
    "    hidden_size: int = 768\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(self.config.vocab_size, self.config.embd_size),\n",
    "            wpe = nn.Embedding(self.config.context_length, self.config.embd_size),\n",
    "            h = nn.ModuleList([Block(self.config) for _ in range(self.config.num_layers)]),\n",
    "            ln_f = nn.LayerNorm(self.config.embd_size)\n",
    "        ))\n",
    "        self.transformer.wte.weight =  nn.Linear(self.config.embd_size, self.config.vocab_size, bias=False).weight\n",
    "        \n",
    "        # init params (iterates over all submodules and applies _init_weights)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'SCALE_INIT'):\n",
    "                std /= (2 * self.config.num_layers)**0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0, std=std)    # as per openai gpt-2 source code\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.config.context_length, f'sequence length {T} should be <= {self.config.context_length}'\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)    # (T,)\n",
    "        pos_embd = self.transformer.wpe(pos)    # (T, embd_size)\n",
    "        tok_embd = self.transformer.wte(idx)    # (B, T, embd_size)\n",
    "        x = pos_embd + tok_embd    # (B, T, embd_size)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)    # (B, T, embd_size)\n",
    "        # logits = self.lm_head(x)    # (B, T, vocab_size)\n",
    "        \n",
    "        # loss = None\n",
    "        # if targets is not None:\n",
    "        #     loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1))\n",
    "        # return logits, loss\n",
    "        return x\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\" Loads pretrained GPT2 model weights from huggingface \"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'uer/gpt2-distil-chinese-cluecorpussmall'}\n",
    "        from transformers import GPT2Model, GPT2LMHeadModel\n",
    "        print(f\"loading weights from pretrained gpt: {model_type}\")\n",
    "\n",
    "        config_args = {\n",
    "            'uer/gpt2-distil-chinese-cluecorpussmall': dict(num_layers=6, num_heads=12, embd_size=768),    # 82M params\n",
    "            'gpt2': dict(num_layers=12, num_heads=12, embd_size=768),    # 124M params\n",
    "            'gpt2-medium': dict(num_layers=24, num_heads=16, embd_size=1024),    # 350M params\n",
    "            'gpt2-large': dict(num_layers=36, num_heads=20, embd_size=1280),    # 774M params\n",
    "            'gpt2-xl': dict(num_layers=48, num_heads=25, embd_size=1600),    # 1558M params\n",
    "        }[model_type]\n",
    "        # config_args['vocab_size'] = 50257\n",
    "        config_args['vocab_size'] = 21128\n",
    "        config_args['context_length'] = 1024\n",
    "\n",
    "        # create a from-scratch minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n",
    "\n",
    "        # init a huggingface transformers model\n",
    "        \n",
    "        # model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        model_hf = GPT2Model.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "\n",
    "        assert len(sd_keys) == len(sd_keys_hf), f\"mismatched keys {len(sd_keys)} != {len(sd_keys_hf)}\"\n",
    "\n",
    "        # copy while ensuring all parameters are aligned in names and shape\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # need to transpose Conv1D weights\n",
    "                assert sd_hf[k].shape[::-1]  == sd[\"transformer.\" + k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[\"transformer.\" + k].copy_(sd_hf[k].T)\n",
    "            else:\n",
    "                assert sd_hf[k].shape == sd[\"transformer.\" + k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[\"transformer.\" + k].copy_(sd_hf[k])\n",
    "        return model\n",
    "\n",
    "class GPT2ForClassification(nn.Module):\n",
    "    def __init__(self, base_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.gpt2 = base_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifer = nn.Linear(base_model.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.gpt2(input_ids)\n",
    "        hidden_state = outputs\n",
    "        last_hidden_state = hidden_state[:, -1, :]\n",
    "        output = self.dropout(last_hidden_state)\n",
    "        logits = self.classifer(output)\n",
    "        return logits\n",
    "        \n",
    "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    acc_num = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch in validloader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            output = model(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "            pred = torch.argmax(output, dim=-1)\n",
    "            acc_num += (pred.long() == batch[\"sentiments\"].long()).float().sum()\n",
    "    return acc_num / len(validset)\n",
    "\n",
    "def train(model, epoch=3, log_step=100, optimizer=None):\n",
    "    global_step = 0\n",
    "    critierion = nn.CrossEntropyLoss()\n",
    "    for ep in trange(epoch):\n",
    "        model.train()\n",
    "\n",
    "        for batch in trainloader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "            loss = critierion(logits, batch[\"sentiments\"])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if global_step % log_step == 0:\n",
    "                print(f\"ep: {ep+1}, global_step: {global_step}, loss: {loss.item()}\")\n",
    "                \n",
    "            global_step += 1\n",
    "        acc = evaluate()\n",
    "        print(f\"ep: {ep+1}, acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig()\n",
    "baseModel = GPT(config)\n",
    "model = GPT2ForClassification(baseModel, 2).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 1, global_step: 0, loss: 0.7901384234428406\n",
      "ep: 1, global_step: 100, loss: 0.7122424840927124\n",
      "ep: 1, global_step: 200, loss: 0.5884904861450195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:25<02:55, 25.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 1, acc: 0.7479674816131592\n",
      "ep: 2, global_step: 300, loss: 0.3923863470554352\n",
      "ep: 2, global_step: 400, loss: 0.26907235383987427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:49<02:29, 24.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 2, acc: 0.7886178493499756\n",
      "ep: 3, global_step: 500, loss: 0.3506380319595337\n",
      "ep: 3, global_step: 600, loss: 0.265786737203598\n",
      "ep: 3, global_step: 700, loss: 0.2473343312740326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [01:14<02:04, 24.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 3, acc: 0.9047619104385376\n",
      "ep: 4, global_step: 800, loss: 0.2590174078941345\n",
      "ep: 4, global_step: 900, loss: 0.04546459764242172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [01:40<01:41, 25.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 4, acc: 0.9500580430030823\n",
      "ep: 5, global_step: 1000, loss: 0.038341064006090164\n",
      "ep: 5, global_step: 1100, loss: 0.04042918607592583\n",
      "ep: 5, global_step: 1200, loss: 0.13934391736984253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [02:10<01:20, 26.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 5, acc: 0.9465737342834473\n",
      "ep: 6, global_step: 1300, loss: 0.035134296864271164\n",
      "ep: 6, global_step: 1400, loss: 0.19272421300411224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [02:40<00:55, 27.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 6, acc: 0.9628338813781738\n",
      "ep: 7, global_step: 1500, loss: 0.0009227603441104293\n",
      "ep: 7, global_step: 1600, loss: 0.01415631826967001\n",
      "ep: 7, global_step: 1700, loss: 0.0009144622599706054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [03:12<00:29, 29.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 7, acc: 0.9709639549255371\n",
      "ep: 8, global_step: 1800, loss: 0.0060768830589950085\n",
      "ep: 8, global_step: 1900, loss: 0.0030458541586995125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [03:44<00:00, 28.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 8, acc: 0.9767711758613586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "train(model, 8, 100, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input：呵呵，一起来围观一下吧。\n",
      "Prediction:好评！\n"
     ]
    }
   ],
   "source": [
    "comment = \"呵呵，一起来围观一下吧。\"\n",
    "id2_label = {0: \"差评！\", 1: \"好评！\"}\n",
    "model.eval()\n",
    "# tokenizer(sen)['input_ids']\n",
    "with torch.inference_mode():\n",
    "    inputs = tokenizer(comment, return_tensors=\"pt\")\n",
    "    # inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    logits = model(inputs['input_ids'].cuda(), attention_mask=inputs['attention_mask'].cuda())\n",
    "    pred = torch.argmax(logits, dim=1)\n",
    "    print(f\"Input：{comment}\\nPrediction:{id2_label.get(pred.item())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"GPT2-small_with_classification_head.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: uer/gpt2-distil-chinese-cluecorpussmall\n"
     ]
    }
   ],
   "source": [
    "config = GPTConfig()\n",
    "baseModel = GPT(config)\n",
    "baseModel = baseModel.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\")\n",
    "\n",
    "model = GPT2ForClassification(baseModel, 2).to(device)\n",
    "\"Only train the last layer\"\n",
    "for param in model.gpt2.parameters():\n",
    "    param.requires_grad = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 1, global_step: 0, loss: 1.0900163650512695\n",
      "ep: 1, global_step: 100, loss: 0.6946725249290466\n",
      "ep: 1, global_step: 200, loss: 0.7056988477706909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:03<00:55,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 1, acc: 0.5121951103210449\n",
      "ep: 2, global_step: 300, loss: 0.6752122044563293\n",
      "ep: 2, global_step: 400, loss: 0.7305796146392822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:07<00:51,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 2, acc: 0.5180023312568665\n",
      "ep: 3, global_step: 500, loss: 0.6500470042228699\n",
      "ep: 3, global_step: 600, loss: 0.6702553629875183\n",
      "ep: 3, global_step: 700, loss: 0.6600936651229858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:11<00:46,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 3, acc: 0.5191637277603149\n",
      "ep: 4, global_step: 800, loss: 0.6901057958602905\n",
      "ep: 4, global_step: 900, loss: 0.6669338345527649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [00:15<00:42,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 4, acc: 0.5133565664291382\n",
      "ep: 5, global_step: 1000, loss: 0.6859195232391357\n",
      "ep: 5, global_step: 1100, loss: 0.6924697160720825\n",
      "ep: 5, global_step: 1200, loss: 0.6346258521080017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [00:19<00:38,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 5, acc: 0.5238094925880432\n",
      "ep: 6, global_step: 1300, loss: 0.7667348980903625\n",
      "ep: 6, global_step: 1400, loss: 0.6618183851242065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:23<00:34,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 6, acc: 0.5354239344596863\n",
      "ep: 7, global_step: 1500, loss: 0.7271660566329956\n",
      "ep: 7, global_step: 1600, loss: 0.7051323652267456\n",
      "ep: 7, global_step: 1700, loss: 0.6964710354804993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [00:27<00:31,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 7, acc: 0.5331010222434998\n",
      "ep: 8, global_step: 1800, loss: 0.6328598856925964\n",
      "ep: 8, global_step: 1900, loss: 0.6865546703338623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [00:31<00:27,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 8, acc: 0.5400696992874146\n",
      "ep: 9, global_step: 2000, loss: 0.7053009271621704\n",
      "ep: 9, global_step: 2100, loss: 0.6811754703521729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [00:35<00:23,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 9, acc: 0.5400696992874146\n",
      "ep: 10, global_step: 2200, loss: 0.6535419225692749\n",
      "ep: 10, global_step: 2300, loss: 0.7783303260803223\n",
      "ep: 10, global_step: 2400, loss: 0.7006934285163879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [00:39<00:20,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 10, acc: 0.5481997728347778\n",
      "ep: 11, global_step: 2500, loss: 0.699901282787323\n",
      "ep: 11, global_step: 2600, loss: 0.6970802545547485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [00:44<00:17,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 11, acc: 0.5493611693382263\n",
      "ep: 12, global_step: 2700, loss: 0.6449683308601379\n",
      "ep: 12, global_step: 2800, loss: 0.6801198124885559\n",
      "ep: 12, global_step: 2900, loss: 0.6923301219940186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [00:49<00:13,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 12, acc: 0.5493611693382263\n",
      "ep: 13, global_step: 3000, loss: 0.6453996896743774\n",
      "ep: 13, global_step: 3100, loss: 0.6626791954040527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [00:53<00:08,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 13, acc: 0.5598141551017761\n",
      "ep: 14, global_step: 3200, loss: 0.6464694738388062\n",
      "ep: 14, global_step: 3300, loss: 0.6497233510017395\n",
      "ep: 14, global_step: 3400, loss: 0.6897488236427307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [00:58<00:04,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 14, acc: 0.5540069341659546\n",
      "ep: 15, global_step: 3500, loss: 0.6780378818511963\n",
      "ep: 15, global_step: 3600, loss: 0.7538015246391296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [01:03<00:00,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 15, acc: 0.5656213760375977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "train(model, 15, 100, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
